{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd \nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport keras.backend as K\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\nimport seaborn as sns\n\nseed = 232\nnp.random.seed(seed)\ntf.random.set_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:01:08.257730Z","iopub.execute_input":"2022-05-31T21:01:08.258256Z","iopub.status.idle":"2022-05-31T21:01:08.271899Z","shell.execute_reply.started":"2022-05-31T21:01:08.258215Z","shell.execute_reply":"2022-05-31T21:01:08.270781Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"img_size = 150\nlabels = [\"NORMAL\",\"PNEUMONIA\"]\n\ndef data_prep(data_dir):\n    data =[]\n    counter = 0\n    data1=[]\n    data2=[]\n    for label in labels:\n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n#             print(os.path.join(path,img))\n#             clear_output(wait=True)\n            if 'test' in path:\n                if(img.endswith('.jpeg')):\n                    \n                    img_arr = cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)\n                    resize_arr = cv2.resize(img_arr,(img_size,img_size))\n                    data.append([resize_arr,class_num])\n                    if counter%2==0:\n                        data1.append([resize_arr,class_num])\n                    else:\n                        data2.append(([resize_arr,class_num]))\n                    counter+=1\n            if(img.endswith('.jpeg')):\n                img_arr = cv2.imread(os.path.join(path,img),cv2.IMREAD_COLOR)\n                resize_arr = cv2.resize(img_arr,(img_size,img_size))\n                data.append([resize_arr,class_num])\n    if 'test' in path:\n        return np.array(data1), np.array(data2)\n    else: \n        return np.array(data)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:01:08.274316Z","iopub.execute_input":"2022-05-31T21:01:08.275071Z","iopub.status.idle":"2022-05-31T21:01:08.292379Z","shell.execute_reply.started":"2022-05-31T21:01:08.275021Z","shell.execute_reply":"2022-05-31T21:01:08.291401Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train = data_prep('../input/chest-xray-pneumonia//chest_xray/chest_xray/train')\ntest1, test2 = data_prep('../input/chest-xray-pneumonia//chest_xray/chest_xray/test')\nval = data_prep('../input/chest-xray-pneumonia//chest_xray/chest_xray/val')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:01:08.294221Z","iopub.execute_input":"2022-05-31T21:01:08.294881Z","iopub.status.idle":"2022-05-31T21:02:54.698469Z","shell.execute_reply.started":"2022-05-31T21:01:08.294832Z","shell.execute_reply":"2022-05-31T21:02:54.697448Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = []\ny_train = []\n\nx_test = []\ny_test = []\n\nx_test2 = []\ny_test2 = []\n\nx_val = []\ny_val = []\n\nfor features, label in train:\n    x_train.append(features)\n    y_train.append(label)\n    \nfor features, label in test1:\n    x_test.append(features)\n    y_test.append(label)\n\nfor features,label in test2:\n    x_test2.append(features)\n    y_test2.append(label)\n    \nfor features, label in val:\n    x_val.append(features)\n    y_val.append(label)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:02:54.700877Z","iopub.execute_input":"2022-05-31T21:02:54.701271Z","iopub.status.idle":"2022-05-31T21:02:54.721690Z","shell.execute_reply.started":"2022-05-31T21:02:54.701235Z","shell.execute_reply":"2022-05-31T21:02:54.720862Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Normalizing the data\nx_train = np.array(x_train) / 255\nx_test = np.array(x_test) / 255\nx_test2 = np.array(x_test2)/255\nx_val = np.array(x_val) / 255","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:02:54.723367Z","iopub.execute_input":"2022-05-31T21:02:54.723786Z","iopub.status.idle":"2022-05-31T21:02:56.971006Z","shell.execute_reply.started":"2022-05-31T21:02:54.723747Z","shell.execute_reply":"2022-05-31T21:02:56.969953Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Reshaping the data\n# Note: using greyscale image that is why used 1 if it was RGB image then it would be 3\n\nx_train = x_train.reshape(-1,img_size,img_size,3)\ny_train = np.array(y_train)\ny_train = y_train.reshape(-1,1)\n\nx_test = x_test.reshape(-1,img_size,img_size,3)\ny_test = np.array(y_test)\ny_test = y_test.reshape(-1,1)\n\nx_test2 = x_test2.reshape(-1,img_size,img_size,3)\ny_test2 = np.array(y_test2)\ny_test2 = y_test2.reshape(-1,1)\n\nx_val = x_val.reshape(-1,img_size,img_size,3)\ny_val = np.array(y_val)\ny_val = y_val.reshape(-1,1)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:02:56.972418Z","iopub.execute_input":"2022-05-31T21:02:56.972854Z","iopub.status.idle":"2022-05-31T21:02:56.985634Z","shell.execute_reply.started":"2022-05-31T21:02:56.972817Z","shell.execute_reply":"2022-05-31T21:02:56.984067Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator()\n        # featurewise_center=False,  # set input mean to 0 over the dataset\n        # samplewise_center=False,  # set each sample mean to 0\n        # featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        # samplewise_std_normalization=False,  # divide each input by its std\n        # zca_whitening=False,  # apply ZCA whitening\n        # rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n        # zoom_range = 0.2, # Randomly zoom image \n        # width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        # height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        # horizontal_flip = True,  # randomly flip images\n        # vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:02:56.987680Z","iopub.execute_input":"2022-05-31T21:02:56.988912Z","iopub.status.idle":"2022-05-31T21:03:01.224390Z","shell.execute_reply.started":"2022-05-31T21:02:56.988863Z","shell.execute_reply":"2022-05-31T21:03:01.223250Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"img_dims = 150\ninputs = Input(shape=(img_dims, img_dims, 3))\n\n# 1 blok konwolucyjny\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\nx = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n# 2 blok konwolucyjny\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n# 3 blok konwolucyjny\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\n\n\n# 4 blok konwolucyjny\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# 5 blok konwolucyjny\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPool2D(pool_size=(2, 2))(x)\nx = Dropout(rate=0.2)(x)\n\n# Warstwa spłaszczająca\nx = Flatten()(x)\nx = Dense(units=512, activation='relu')(x)\nx = Dropout(rate=0.7)(x)\nx = Dense(units=128, activation='relu')(x)\nx = Dropout(rate=0.5)(x)\nx = Dense(units=64, activation='relu')(x)\nx = Dropout(rate=0.3)(x)\n\n# Output layer\noutput = Dense(units=1, activation='sigmoid')(x)\n\n# Creating model and compiling\nmodel_1 = Model(inputs=inputs, outputs=output)\nmodel_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:03:01.226231Z","iopub.execute_input":"2022-05-31T21:03:01.226553Z","iopub.status.idle":"2022-05-31T21:03:01.509493Z","shell.execute_reply.started":"2022-05-31T21:03:01.226524Z","shell.execute_reply":"2022-05-31T21:03:01.508195Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"STEPS = len(x_train) / 32\nhist = model_1.fit(\n           datagen.flow(x_train,y_train,batch_size = 32), steps_per_epoch=STEPS, \n           epochs=10, validation_data=datagen.flow(x_test2, y_test2),\n              callbacks=[checkpoint, lr_reduce])\n#datagen.flow(x_train,y_train,batch_size = 32), steps_per_epoch=STEPS, batch_size = 32, epochs=10, validation_data = datagen.flow(x_val, y_val),","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:03:01.511614Z","iopub.execute_input":"2022-05-31T21:03:01.512220Z","iopub.status.idle":"2022-05-31T21:12:11.876736Z","shell.execute_reply.started":"2022-05-31T21:03:01.512175Z","shell.execute_reply":"2022-05-31T21:12:11.875769Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/10\n163/163 [==============================] - 59s 341ms/step - loss: 0.3301 - accuracy: 0.8556 - val_loss: 0.7990 - val_accuracy: 0.6250\nEpoch 2/10\n163/163 [==============================] - 55s 336ms/step - loss: 0.1411 - accuracy: 0.9526 - val_loss: 1.4722 - val_accuracy: 0.6250\nEpoch 3/10\n163/163 [==============================] - 53s 328ms/step - loss: 0.0946 - accuracy: 0.9640 - val_loss: 2.5751 - val_accuracy: 0.6250\nEpoch 4/10\n163/163 [==============================] - 54s 333ms/step - loss: 0.0867 - accuracy: 0.9701 - val_loss: 2.0526 - val_accuracy: 0.6250\nEpoch 5/10\n163/163 [==============================] - 55s 334ms/step - loss: 0.0740 - accuracy: 0.9726 - val_loss: 1.6430 - val_accuracy: 0.6891\n\nEpoch 00005: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 6/10\n163/163 [==============================] - 54s 330ms/step - loss: 0.0480 - accuracy: 0.9843 - val_loss: 1.3774 - val_accuracy: 0.7276\nEpoch 7/10\n163/163 [==============================] - 56s 341ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 1.3320 - val_accuracy: 0.7628\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\nEpoch 8/10\n163/163 [==============================] - 55s 337ms/step - loss: 0.0250 - accuracy: 0.9914 - val_loss: 1.4527 - val_accuracy: 0.7724\nEpoch 9/10\n163/163 [==============================] - 54s 334ms/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 1.7862 - val_accuracy: 0.7436\n\nEpoch 00009: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\nEpoch 10/10\n163/163 [==============================] - 55s 336ms/step - loss: 0.0148 - accuracy: 0.9954 - val_loss: 1.6602 - val_accuracy: 0.7628\n","output_type":"stream"}]},{"cell_type":"code","source":"model_1.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:20.470576Z","iopub.execute_input":"2022-05-31T21:12:20.471067Z","iopub.status.idle":"2022-05-31T21:12:21.803006Z","shell.execute_reply.started":"2022-05-31T21:12:20.471030Z","shell.execute_reply":"2022-05-31T21:12:21.802075Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"10/10 [==============================] - 1s 78ms/step - loss: 1.5140 - accuracy: 0.7756\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[1.5140011310577393, 0.7756410241127014]"},"metadata":{}}]},{"cell_type":"code","source":"model_1.save('model_1.tf')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:25.780699Z","iopub.execute_input":"2022-05-31T21:12:25.781151Z","iopub.status.idle":"2022-05-31T21:12:31.404848Z","shell.execute_reply.started":"2022-05-31T21:12:25.781112Z","shell.execute_reply":"2022-05-31T21:12:31.403735Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nResNet-18\nReference:\n[1] K. He et al. Deep Residual Learning for Image Recognition. CVPR, 2016\n[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:\nSurpassing human-level performance on imagenet classification. In\nICCV, 2015.\n\"\"\"\n\n\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense, Conv2D,  MaxPool2D, Flatten, GlobalAveragePooling2D,  BatchNormalization, Layer, Add\nfrom keras.models import Sequential\nfrom keras.models import Model\nimport tensorflow as tf\n\n\nclass ResnetBlock(Model):\n    \"\"\"\n    A standard resnet block.\n    \"\"\"\n\n    def __init__(self, channels: int, down_sample=False):\n        \"\"\"\n        channels: same as number of convolution kernels\n        \"\"\"\n        super().__init__()\n\n        self.__channels = channels\n        self.__down_sample = down_sample\n        self.__strides = [2, 1] if down_sample else [1, 1]\n\n        KERNEL_SIZE = (3, 3)\n        # use He initialization, instead of Xavier (a.k.a 'glorot_uniform' in Keras), as suggested in [2]\n        INIT_SCHEME = \"he_normal\"\n\n        self.conv_1 = Conv2D(self.__channels, strides=self.__strides[0],\n                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n        self.bn_1 = BatchNormalization()\n        self.conv_2 = Conv2D(self.__channels, strides=self.__strides[1],\n                             kernel_size=KERNEL_SIZE, padding=\"same\", kernel_initializer=INIT_SCHEME)\n        self.bn_2 = BatchNormalization()\n        self.merge = Add()\n\n        if self.__down_sample:\n            # perform down sampling using stride of 2, according to [1].\n            self.res_conv = Conv2D(\n                self.__channels, strides=2, kernel_size=(1, 1), kernel_initializer=INIT_SCHEME, padding=\"same\")\n            self.res_bn = BatchNormalization()\n\n    def call(self, inputs):\n        res = inputs\n\n        x = self.conv_1(inputs)\n        x = self.bn_1(x)\n        x = tf.nn.relu(x)\n        x = self.conv_2(x)\n        x = self.bn_2(x)\n\n        if self.__down_sample:\n            res = self.res_conv(res)\n            res = self.res_bn(res)\n\n        # if not perform down sample, then add a shortcut directly\n        x = self.merge([x, res])\n        out = tf.nn.relu(x)\n        return out\n\n\nclass ResNet18(Model):\n\n    def __init__(self, num_classes, **kwargs):\n        \"\"\"\n            num_classes: number of classes in specific classification task.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.conv_1 = Conv2D(64, (7, 7), strides=2,\n                             padding=\"same\", kernel_initializer=\"he_normal\")\n        self.init_bn = BatchNormalization()\n        self.pool_2 = MaxPool2D(pool_size=(2, 2), strides=2, padding=\"same\")\n        self.res_1_1 = ResnetBlock(64)\n        self.res_1_2 = ResnetBlock(64)\n        self.res_2_1 = ResnetBlock(128, down_sample=True)\n        self.res_2_2 = ResnetBlock(128)\n        self.res_3_1 = ResnetBlock(256, down_sample=True)\n        self.res_3_2 = ResnetBlock(256)\n        self.res_4_1 = ResnetBlock(512, down_sample=True)\n        self.res_4_2 = ResnetBlock(512)\n        self.avg_pool = GlobalAveragePooling2D()\n        self.flat = Flatten()\n        self.fc = Dense(num_classes, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        out = self.conv_1(inputs)\n        out = self.init_bn(out)\n        out = tf.nn.relu(out)\n        out = self.pool_2(out)\n        for res_block in [self.res_1_1, self.res_1_2, self.res_2_1, self.res_2_2, self.res_3_1, self.res_3_2, self.res_4_1, self.res_4_2]:\n            out = res_block(out)\n        out = self.avg_pool(out)\n        out = self.flat(out)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:36.228103Z","iopub.execute_input":"2022-05-31T21:12:36.228516Z","iopub.status.idle":"2022-05-31T21:12:36.253106Z","shell.execute_reply.started":"2022-05-31T21:12:36.228483Z","shell.execute_reply":"2022-05-31T21:12:36.252344Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"model_2 = ResNet18(1)\nmodel_2.build(input_shape = (None,150,150,3))\nmodel_2.compile(optimizer='adam',\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\nmodel_2.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T21:12:41.902565Z","iopub.execute_input":"2022-05-31T21:12:41.903030Z","iopub.status.idle":"2022-05-31T21:12:42.496906Z","shell.execute_reply.started":"2022-05-31T21:12:41.902993Z","shell.execute_reply":"2022-05-31T21:12:42.495805Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"WARNING: AutoGraph could not transform <bound method ResnetBlock.call of <__main__.ResnetBlock object at 0x7f7814b3b450>> and will run it as-is.\nCause: mangled names are not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\nModel: \"res_net18_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_22 (Conv2D)           multiple                  9472      \n_________________________________________________________________\nbatch_normalization_24 (Batc multiple                  256       \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 multiple                  0         \n_________________________________________________________________\nresnet_block_8 (ResnetBlock) multiple                  74368     \n_________________________________________________________________\nresnet_block_9 (ResnetBlock) multiple                  74368     \n_________________________________________________________________\nresnet_block_10 (ResnetBlock multiple                  231296    \n_________________________________________________________________\nresnet_block_11 (ResnetBlock multiple                  296192    \n_________________________________________________________________\nresnet_block_12 (ResnetBlock multiple                  921344    \n_________________________________________________________________\nresnet_block_13 (ResnetBlock multiple                  1182208   \n_________________________________________________________________\nresnet_block_14 (ResnetBlock multiple                  3677696   \n_________________________________________________________________\nresnet_block_15 (ResnetBlock multiple                  4723712   \n_________________________________________________________________\nglobal_average_pooling2d_1 ( multiple                  0         \n_________________________________________________________________\nflatten_2 (Flatten)          multiple                  0         \n_________________________________________________________________\ndense_5 (Dense)              multiple                  513       \n=================================================================\nTotal params: 11,191,425\nTrainable params: 11,181,825\nNon-trainable params: 9,600\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                            patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\nSTEPS = len(x_train) / 32\n# hist = model_2.fit(\n#            datagen.flow(x_train,y_train,batch_size = 32), steps_per_epoch=STEPS, \n#            epochs=10, validation_data=datagen.flow(x_test2, y_test2),\n#               callbacks=[learning_rate_reduction])\nhist = model_2.fit(\n           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n           epochs=epochs, validation_data=test_gen,\n           validation_steps=test_gen.samples // batch_size, callbacks=[checkpoint, lr_reduce])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T23:08:37.122546Z","iopub.execute_input":"2022-05-31T23:08:37.123282Z","iopub.status.idle":"2022-06-01T00:08:36.386782Z","shell.execute_reply.started":"2022-05-31T23:08:37.123234Z","shell.execute_reply":"2022-06-01T00:08:36.385723Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1/10\n163/163 [==============================] - 360s 2s/step - loss: 0.3349 - accuracy: 0.8832 - val_loss: 0.2240 - val_accuracy: 0.9178\nEpoch 2/10\n163/163 [==============================] - 355s 2s/step - loss: 0.1967 - accuracy: 0.9210 - val_loss: 0.2732 - val_accuracy: 0.9079\nEpoch 3/10\n163/163 [==============================] - 357s 2s/step - loss: 0.1623 - accuracy: 0.9369 - val_loss: 0.3079 - val_accuracy: 0.8947\nEpoch 4/10\n163/163 [==============================] - 357s 2s/step - loss: 0.1518 - accuracy: 0.9385 - val_loss: 0.3388 - val_accuracy: 0.9112\nEpoch 5/10\n163/163 [==============================] - 357s 2s/step - loss: 0.1460 - accuracy: 0.9431 - val_loss: 0.4403 - val_accuracy: 0.8766\nEpoch 6/10\n163/163 [==============================] - 356s 2s/step - loss: 0.1416 - accuracy: 0.9440 - val_loss: 0.4738 - val_accuracy: 0.8701\nEpoch 7/10\n163/163 [==============================] - 356s 2s/step - loss: 0.1304 - accuracy: 0.9503 - val_loss: 0.2523 - val_accuracy: 0.9112\nEpoch 8/10\n163/163 [==============================] - 356s 2s/step - loss: 0.1250 - accuracy: 0.9509 - val_loss: 0.6800 - val_accuracy: 0.8141\nEpoch 9/10\n163/163 [==============================] - 357s 2s/step - loss: 0.1134 - accuracy: 0.9553 - val_loss: 0.1894 - val_accuracy: 0.9293\nEpoch 10/10\n163/163 [==============================] - 361s 2s/step - loss: 0.1143 - accuracy: 0.9569 - val_loss: 0.3416 - val_accuracy: 0.8931\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_2.evaluate(x_test, y_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T00:10:24.444162Z","iopub.execute_input":"2022-06-01T00:10:24.444999Z","iopub.status.idle":"2022-06-01T00:10:29.749308Z","shell.execute_reply.started":"2022-06-01T00:10:24.444954Z","shell.execute_reply":"2022-06-01T00:10:29.748550Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"10/10 [==============================] - 5s 515ms/step - loss: 0.3084 - accuracy: 0.8974\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"[0.3083782494068146, 0.8974359035491943]"},"metadata":{}}]},{"cell_type":"code","source":"model_2.save('model_2.tf')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T00:10:33.798811Z","iopub.execute_input":"2022-06-01T00:10:33.799575Z","iopub.status.idle":"2022-06-01T00:10:49.628347Z","shell.execute_reply.started":"2022-06-01T00:10:33.799520Z","shell.execute_reply":"2022-06-01T00:10:49.626810Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"model_1.save('model_1.tf')\nmodel_2.save('model_2.tf')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T23:31:18.130586Z","iopub.execute_input":"2022-05-30T23:31:18.131047Z","iopub.status.idle":"2022-05-30T23:31:36.227892Z","shell.execute_reply.started":"2022-05-30T23:31:18.131007Z","shell.execute_reply":"2022-05-30T23:31:36.226818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.datasets import make_blobs\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom numpy import dstack\n \n# load models from file\ndef load_all_models(n_models):\n\tall_models = list()\n\tfor i in range(n_models):\n\t\t# define filename for this ensemble\n\t\tfilename = 'model_' + str(i + 1) + '.tf'\n\t\t# load model from file\n\t\tmodel = load_model(filename)\n\t\t# add to list of members\n\t\tall_models.append(model)\n\t\tprint('>loaded %s' % filename)\n\treturn all_models\n \n# create stacked model input dataset as outputs from the ensemble\ndef stacked_dataset(members, inputX):\n\tstackX = None\n\tfor model in members:\n\t\t# make prediction\n\t\tyhat = model.predict(inputX, verbose=0)\n\t\t# stack predictions into [rows, members, probabilities]\n\t\tif stackX is None:\n\t\t\tstackX = yhat\n\t\telse:\n\t\t\tstackX = dstack((stackX, yhat))\n\t# flatten predictions to [rows, members x probabilities]\n\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n\treturn stackX\n \n# fit a model based on the outputs from the ensemble members\ndef fit_stacked_model(members, inputX, inputy):\n\t# create dataset using ensemble\n\tstackedX = stacked_dataset(members, inputX)\n\t# fit standalone model\n\tmodel = LogisticRegression()\n\tmodel.fit(stackedX, inputy)\n\treturn model\n \n# make a prediction with the stacked model\ndef stacked_prediction(members, model, inputX):\n\t# create dataset using ensemble\n\tstackedX = stacked_dataset(members, inputX)\n\t# make a prediction\n\tyhat = model.predict(stackedX)\n\treturn yhat\n \n# generate 2d classification dataset\nX, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n# split into train and test\n# n_train = 100\n# trainX, testX = X[:n_train, :], X[n_train:, :]\n# trainy, testy = y[:n_train], y[n_train:]\n# print(trainX.shape, testX.shape)\ntrainX,testX = x_train, x_test\ntrainy, testy = y_train, y_test\n# load all models\nn_members = 3\nmembers = load_all_models(n_members)\nprint('Loaded %d models' % len(members))\n# evaluate standalone models on test dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T00:11:53.507304Z","iopub.execute_input":"2022-06-01T00:11:53.507762Z","iopub.status.idle":"2022-06-01T00:12:03.596136Z","shell.execute_reply.started":"2022-06-01T00:11:53.507723Z","shell.execute_reply":"2022-06-01T00:12:03.595000Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":">loaded model_1.tf\n>loaded model_2.tf\n>loaded model_3.tf\nLoaded 3 models\n","output_type":"stream"}]},{"cell_type":"code","source":"# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\nfor model in members:\n\ttesty_enc = to_categorical(testy, num_classes=2)\n\t_, acc = model.evaluate(testX, testy)\n\tprint('Model Accuracy: %.3f' % acc)\n# fit stacked model using the ensemble\nmodel = fit_stacked_model(members, testX, testy)\n# evaluate model on test set\nyhat = stacked_prediction(members, model, testX)\nacc = accuracy_score(testy, yhat)\nprint('Stacked Test Accuracy: %.3f' % acc)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T00:12:10.713758Z","iopub.execute_input":"2022-06-01T00:12:10.714419Z","iopub.status.idle":"2022-06-01T00:12:34.316160Z","shell.execute_reply.started":"2022-06-01T00:12:10.714374Z","shell.execute_reply":"2022-06-01T00:12:34.315231Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"10/10 [==============================] - 1s 77ms/step - loss: 1.5140 - accuracy: 0.7756\nModel Accuracy: 0.776\n10/10 [==============================] - 6s 500ms/step - loss: 0.3084 - accuracy: 0.8974\nModel Accuracy: 0.897\n10/10 [==============================] - 1s 77ms/step - loss: 0.1946 - accuracy: 0.9423\nModel Accuracy: 0.942\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"Stacked Test Accuracy: 0.952\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef data_aug(img_dimensions, batch_size, class_mode):\n    #manipulacja danymi\n    #vertical_flip=True\n    train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, rotation_range=10,width_shift_range=0.2, height_shift_range=0.2,brightness_range=[0.4,1.5])\n    test_val_datagen = ImageDataGenerator(rescale=1./255)\n    \n    # generator danych jest wejściem do sieci\n    #generator danych treningowych \n    train_gen = train_datagen.flow_from_directory(\n    directory=input_path+'train', \n    target_size=(img_dimensions, img_dimensions), \n    batch_size=batch_size, \n    class_mode=class_mode, \n    shuffle=True)\n\n    #generator danych testowych\n    test_gen = test_val_datagen.flow_from_directory(\n    directory=input_path+'test', \n    target_size=(img_dimensions, img_dimensions), \n    batch_size=batch_size, \n    class_mode=class_mode, \n    shuffle=True)\n    \n    # zbiór testowy do obliczania metryk\n    test_data = []\n    test_labels = []\n    \n\n    for cond in ['/NORMAL/', '/PNEUMONIA/']:\n        for img in (os.listdir(input_path + 'test' + cond)):\n            img = plt.imread(input_path+'test'+cond+img)\n            img = cv2.resize(img, (img_dimensions, img_dimensions))\n            img = np.dstack([img, img, img])\n            img = img.astype('float32') / 255\n            if cond=='/NORMAL/':\n                label = 0\n            elif cond=='/PNEUMONIA/':\n                label = 1\n            test_data.append(img)\n            test_labels.append(label)\n        \n    test_data = np.array(test_data)\n    test_labels = np.array(test_labels)\n    return train_gen, test_gen, test_data, test_labels","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:37:36.637666Z","iopub.execute_input":"2022-05-31T22:37:36.638113Z","iopub.status.idle":"2022-05-31T22:37:36.652828Z","shell.execute_reply.started":"2022-05-31T22:37:36.638074Z","shell.execute_reply":"2022-05-31T22:37:36.651621Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"input_path = '../input/chest-xray-pneumonia//chest_xray/chest_xray/'\nimg_dims = 150\nepochs = 10\nbatch_size = 32\ntrain_gen, test_gen, test_data, test_labels = data_aug(img_dims, batch_size, \"binary\")","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:39:11.348722Z","iopub.execute_input":"2022-05-31T22:39:11.349207Z","iopub.status.idle":"2022-05-31T22:39:19.431249Z","shell.execute_reply.started":"2022-05-31T22:39:11.349167Z","shell.execute_reply":"2022-05-31T22:39:19.430150Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Found 5216 images belonging to 2 classes.\nFound 624 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_3 = Model(inputs=inputs, outputs=output)\nmodel_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:38:24.869022Z","iopub.execute_input":"2022-05-31T22:38:24.869440Z","iopub.status.idle":"2022-05-31T22:38:24.889539Z","shell.execute_reply.started":"2022-05-31T22:38:24.869403Z","shell.execute_reply":"2022-05-31T22:38:24.888750Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"hist = model_3.fit(\n           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n           epochs=epochs, validation_data=test_gen,\n           validation_steps=test_gen.samples // batch_size, callbacks=[checkpoint, lr_reduce])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:40:26.304164Z","iopub.execute_input":"2022-05-31T22:40:26.304682Z","iopub.status.idle":"2022-05-31T23:05:37.420067Z","shell.execute_reply.started":"2022-05-31T22:40:26.304605Z","shell.execute_reply":"2022-05-31T23:05:37.419254Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1/10\n163/163 [==============================] - 151s 927ms/step - loss: 0.3765 - accuracy: 0.8499 - val_loss: 0.3542 - val_accuracy: 0.8832\nEpoch 2/10\n163/163 [==============================] - 145s 890ms/step - loss: 0.2938 - accuracy: 0.8748 - val_loss: 0.9668 - val_accuracy: 0.6234\nEpoch 3/10\n163/163 [==============================] - 145s 890ms/step - loss: 0.2659 - accuracy: 0.8959 - val_loss: 0.2467 - val_accuracy: 0.9161\nEpoch 4/10\n163/163 [==============================] - 145s 887ms/step - loss: 0.2309 - accuracy: 0.9055 - val_loss: 0.2246 - val_accuracy: 0.9062\n\nEpoch 00004: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\nEpoch 5/10\n163/163 [==============================] - 145s 888ms/step - loss: 0.2055 - accuracy: 0.9250 - val_loss: 0.2178 - val_accuracy: 0.9211\nEpoch 6/10\n163/163 [==============================] - 144s 882ms/step - loss: 0.1980 - accuracy: 0.9229 - val_loss: 0.2049 - val_accuracy: 0.9227\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\nEpoch 7/10\n163/163 [==============================] - 146s 893ms/step - loss: 0.1766 - accuracy: 0.9294 - val_loss: 0.1945 - val_accuracy: 0.9309\nEpoch 8/10\n163/163 [==============================] - 146s 890ms/step - loss: 0.1716 - accuracy: 0.9337 - val_loss: 0.1954 - val_accuracy: 0.9293\n\nEpoch 00008: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\nEpoch 9/10\n163/163 [==============================] - 146s 896ms/step - loss: 0.1696 - accuracy: 0.9356 - val_loss: 0.1975 - val_accuracy: 0.9309\nEpoch 10/10\n163/163 [==============================] - 147s 901ms/step - loss: 0.1561 - accuracy: 0.9423 - val_loss: 0.2046 - val_accuracy: 0.9342\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_3.save('model_3.tf')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T23:05:48.025226Z","iopub.execute_input":"2022-05-31T23:05:48.025691Z","iopub.status.idle":"2022-05-31T23:05:53.694018Z","shell.execute_reply.started":"2022-05-31T23:05:48.025606Z","shell.execute_reply":"2022-05-31T23:05:53.692904Z"},"trusted":true},"execution_count":44,"outputs":[]}]}